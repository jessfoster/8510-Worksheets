---
title: 'Worksheet 6: Text Analysis'
author: 'Jessica Foster'
date: '2023-04-04'
---

_This is the sixth in a series of worksheets for History 8510 at Clemson University. The goal of these worksheets is simple: practice, practice, practice. The worksheet introduces concepts and techniques and includes prompts for you to practice in this interactive document. When you are finished, you should change the author name (above), knit your document, and upload it to canvas. Don't forget to commit your changes as you go and push to github when you finish the worksheet._

Text analysis is an umbrella for a number of different methodologies. Generally speaking, it involves taking a set (or corpus) of textual sources, turning them into data that a computer can understand, and then running calculations and algorithms using that data. Typically, at its most basic level, that involves the counting of words.

**Text analysis can be broken down into 4 general steps:** 

  1. Acquiring a corpus
  2. Preparing the text or Pre-processing
  3. Choosing an analytical tool 
    * (There are many different tools or methods for text analysis. Take a minute and Google each of these methodologies: tf-idf, topic modeling, sentiment analysis, word vector analysis, n-grams)
  4. Analyzing the results
  
In this worksheet we are focusing on basic text analysis. We'll learn how to load textual data into R, how to prepare it, and then how to analyze it using tf-idf or term-frequency according to inverse document frequency. 

Before doing too much, lets load a few relevant libraries. The last few you will likely need to install.
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext) 
library(readtext)
library(widyr)
library(SnowballC)
```


## Acquiring a Corpus

First, lets install the State of the Union package. This package contains text of all the state of the Union addresses from Washington to Trump. Run `install.packages` to install the `sotu` package. 
```{r}
library(sotu)
```

This package includes both the metadata about these speeches in `sotu_meta` and the texts themselves in `sotu_texts`. Lets first look at the metadata associated with this package. 

```{r}
meta <- as.data.frame(sotu_meta)
head(meta)
```

This package also includes a function that will let us write all of the files to disk. This is crucial but also an unusual step because when conducting text analysis in the real world, you will not have an R package filled with the data. Rather you will have to organize the metadata and load the files yourself. Writing these to the disk allows us to practice that step. 

```{r}
file_paths <- sotu_dir(dir = "sotu_files")
head(file_paths)
```

What this does is create a new directory (sotu_files) and adds each State of the Union address as a text file. Notice each speech is its own .txt file that is comprised of just the text of the speech.

(@) Take a look at the directory in your files pane and open one of the documents. 

Now lets load all these texts into R using the `readtext()` function. First look up the documentation for this function and read about it. 
```{r}
sotu_texts <- readtext(file_paths)
```

Take a look at sotu_texts now. Notice that we have two columns, one filled with the text, and one with a document id. 
```{r}
head(sotu_texts, n = 5)
```

Now our textual data is loaded into R but the textual data and the metadata are in two different data frames. Lets combine them. Note that this isn't the way I would typically recommend doing this but its a quirk of the SOTU data. Typically when I create a metadata spreadsheet for a textual dataset I have a column for the file name which makes joining the textual data and metadata together easier. Here, we'll need to sort the dataset so that is alphabetical and then join the two together.

```{r}
sotu_whole <- 
  sotu_meta %>%  
  arrange(president) %>% # sort metadata
  bind_cols(sotu_texts) %>% # combine with texts
  as_tibble() # convert to tibble for better screen viewing

glimpse(sotu_whole)
```

Now our data is loaded into R and its ready to be pre-processed. 

## Pre-Processing 

### Tokenizing

One of the most basic pre-processing techniques for textual data is to tokenize it. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks but, for historians, its common to remove the numbers and punctuation too. To do this we'll create a data frame where each row contains a single word with its metadata as unit of observation.

`tidytext` provides a function called `unnest_tokens().` We can use this to convert our sotu_whole data frame into one that is tokenized. It takes three arguments:
    
    * a tibble or data frame which contains the text
    * the name of the newly created column that will contain the tokens
    * the name of the column within the data frame which contains the text to be tokenized

```{r}
tidy_sotu <- sotu_whole %>%
  unnest_tokens(word, text)

tidy_sotu
```

`unnest_tokens()` also did something else that is really important: it made everything lowercase and took out all punctuation. The function contains options if we wanted to keep those elements, but for our purposes we don't. 

The function `unnest_tokens()` also has an option called token. Tokenizing by word is the default but you could also tokenize by characters, ngrams, lines, or sentences. 

(@)Use the documentation to tokenize the dataset into sentences: 
```{r}
sotu_sentences <- sotu_whole %>%
  unnest_tokens(sentence, text, token = "sentences")

sotu_sentences
```

We've talked about n-grams loosely in class. But lets define it more formally. An n-gram is a contiguous sequence of n items from a given sample of text or speech. The n stands for the number of items. So for example, a bi-gram is sets of two words. 

For example, if I had the string: "Nothing to fear but fear itself" A bi-gram would look like this: 
  Nothing to, to fear, fear but, but fear, fear itself.

A tri-gram would look like this: 
  Nothing to fear, to fear but, but fear itself
  
We can use unnest_tokens() to create n-grams for us. To do that we just have to add an extra option that defines n. 
```{r}
sotu_bigrams <- sotu_whole %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

head(sotu_bigrams$bigram)
```

(@) Use `unest_tokens()` to create tri-grams. 
```{r}
sotu_trigrams <- sotu_whole %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

head(sotu_trigrams$trigram)
```

### Stopwords

Another crucial component of text analysis is removing stopwords. Stopwords are words like "I, he, she, of, the" that are common and don't convey meaning. Because they are highly common they don't tell us anything about the content of the text itself. 

There are stopwords that come with the `tidytext` package. 
```{r}
stop_words
```
This is just one example of stopwords. You can find other lists such as stopwords in other languages or [stopwords designed specifically for the 19th century.](https://www.matthewjockers.net/macroanalysisbook/expanded-stopwords-list/) Its also possible you may want to edit the list of stopwords to include some of your own. For example, if we wanted to add the word, "America" to the stopwords list we could use add_row to do so: 
```{r}
stop_words_custom <- stop_words %>% add_row(word="America", lexicon="NA")
```

For now lets just remove the default stopwords. The easiest way to do that here is to do an anti-join. We join and return all rows from our table of tokens tidy_sotu where there are no matching values in our list of stopwords. 

```{r}
tidy_sotu_words <- tidy_sotu %>% 
  anti_join(stop_words)

tidy_sotu_words

#another way to do this would be to filter by words NOT in the stop word list like this:  filter(!word %in% stop_words$word)
```

### Stemming 

The third common kind of pre-process is called word stemming. This process reduces a word to its root stem. So for example: fishing becomes fish, fished becomes fish, fishes becomes fish. You can easily see how this might be useful for capturing all forms of a word.

`tidytext` doesn't have its own word stemming function. Instead we have to rely on the functions provided by `hunspell` or `SnowballC`. I prefer `SnowballC`. You may need to install it before running the below code. 

```{r}
library(SnowballC)
tidy_sotu_words %>%
        mutate(word_stem = wordStem(word))
```

Now if you compare the word and word_stem columns you can see the effect that wordStem had. Notice that it works well in cases like 
  
  citizens = citizen 

But it does some odd things to words like representatives. Whether this is useful for you will depend on the question your asking (and the OCR accuracy) but it's a useful technique to be familiar with nevertheless. 

## Analysis

Lets reset our work space and ensure that our df is loaded with single tokenized words and filter by our stopword list. Go ahead and clear your environment using the broom button and then run the below code. This code is simply everything we've run up to this point. 

```{r}
meta <- as.data.frame(sotu_meta)
file_paths <- sotu_dir(dir = "sotu_files")
sotu_texts <- readtext(file_paths)
sotu_whole <- 
  sotu_meta %>%  
  arrange(president) %>% # sort metadata
  bind_cols(sotu_texts) %>% # combine with texts
  as_tibble() 

tidy_sotu <- sotu_whole %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
```
(@) Before we move forward, take a minute a describe the chunk of code you just ran. What does each section do and how does it reflect the workflow for a text analysis project? What are the important steps that are unique to text analysis? 

> First you have to build a corpus. In a typical text analysis project you have to organize the metadata and load the text files yourself but in this case there is an R package already prepared called `sotu`. The `sotu` package includes metadata about each State of the Union address in `sotu_meta`. In the first line of code we are converting `sotu_meta` into a dataframe called `meta`. In the next line, we use the `sotu_dir()` function to create a new directory called `sotu_files` which adds each State of the Union address as its own .txt file. Next we use the `readtext()` function to load the texts into a dataframe called `sotu_texts`. It has two columns, one for the text, and one for the document ID. Now we need to combine the two data frames. Ideally when you're building your corpus you should create a metadata spreadsheet with a column for the file name which makes it easier to join the textual data and metadata, but in this case we have to sort the dataset alphabetically by president before joining the two dataframes into `sotu_whole`. 

> After preparing the corpus, the next step is pre-processing. One of the most basic pre-processing techniques is tokenization, which splits a phrase, sentence, paragraph, or an entire text document into smaller units, called tokens. We can tokenize the `sotu_whole` dataframe using the function `unnest_tokens()`
Here we're creating a dataframe called `tidy_sotu` which has a new column called "word" that contains the tokens. Each row contains a single word with its metadata as the unit of observation. 

> Another pre-processing technique is removing stopwords like "I, he, she, of, the" that are common and don't convey meaning about the content of the text itself. The `tidytext` package comes with a list of `stop_words`. There are other lists available and you can also edit the list to add custom words. In this case we just want to remove the default stopwords. We can use the function `anti_join()` to join and return all rows from `tidy_sotu` where there are no matching values in our list of stopwords.

The most basic kind of analysis we might be interested in doing is counting words. We can do that easily using the `count()` function: 
```{r}
tidy_sotu %>%
  count(word, sort = TRUE)
```

Now we know that the most used word in state of the union speeches is government. But what if we wanted to look at when presidents use the words war versus the word peace? 
```{r}
tidy_sotu %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word)
```

This data frame is too big to understand quickly without visualizing it. We can create a bar chart to better understand it: 
```{r}
library(ggplot2)

tidy_sotu %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill") # Percent stacked barplot: the percentage of each subgroup is represented, allowing to study the evolution of their proportion in the whole

```

We also might want to ask about the average length of each president's State of the Union address. Who had the longest speech and who had the shortest?
```{r}
tidy_sotu %>%
  count(president, doc_id) %>% 
  group_by(president) %>% 
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))

# William Howard Taft had the longest speech, while John Adams had the shortest
```

(@) Think back to the metadata that we loaded about these speeches. Why are more modern presidents' State of the Union addresses shorter? 

> The data shows that the earliest presidents actually had the shortest speeches: John Adams, George Washington, Thomas Jefferson, and James Madison. However, there doesn't seem to be a clear correlation between being a modern president and giving a longer speech either, as you can see that several modern presidents gave short speeches too. I did some internet research and found that presidents usually submitted a written report to Congress until 1913 when Woodrow Wilson started giving the State of the Union in person. This fact still doesn't quite explain the numbers. I think you would need to group the presidents based on some criteria to find out whether a particular "type" of president gave longer or shorter speeches.

> Since I didn't find a clear pattern, I decided to edit the code to include `sotu_type`, that is, whether the State of the Union is categorized as "written" or "speech". See below.

(@) Filter the dataset to address this discrepancy and then recreate these statistics: 
```{r}
tidy_sotu %>%
  count(president, sotu_type, doc_id)  %>% # Add sotu_type to see if correlation between speech/written and length
  group_by(president, sotu_type) %>% # Show sotu_type in summary
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```
> In my opinion, this gives more meaningful results because it's including some metadata, in this case type of speech, that might explain the results. This dataframe shows that the longest speeches are indeed written (the first 20 rows are all "written"). To me, this makes more sense than just saying that modern presidents give longer speeches because first you would need to define "modern", which is a highly contested term. Also, the data simply doesn't bear out the "modern" claim because the longest speeches were given by presidents across time periods. Based on the above results, one could argue that written speeches *tend* to be longer, regardless of the president or time period, but you would need to investigate further to explain why that is. You would also need to find out how the metadata category `sotu_type` was assigned, since "written" is not necessarily the opposite of "speech". For example, even if Ronald Reagan didn't just read aloud his State of the Union address, I would be surprised if he or his speech writer hadn't written it down beforehand. Likewise, I think Washington and Adams probably wrote down their speeches before delivering them, even though they're the shortest.

Just out of curiosity, I want to see which political party of presidents gave the longest speeches:
```{r}
tidy_sotu %>%
  count(president, party, doc_id)  %>% # Add party to see if correlation between party affiliation and length
  group_by(party) %>% # Group by political party
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```
> It turns out that Republicans have given the longest speeches on average, but Democrats and Whigs are a close second and third, respectively. 

### Term Frequency
Often, the raw frequency of a term is not as useful as relative frequency. In other words, how often that word appears relative to the total number of words in a text. This ratio is called **term frequency**. 

You can calculate the term frequency by dividing the total occurrences of a word by the total number of words. Typically you want to do this per document.

Here's an easy way to calculate it: 
```{r}
tidy_sotu_rel.freq <- tidy_sotu %>%
  count(doc_id, word, sort = T)  %>% # count occurrence of word and sort descending
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
```

We can assume that words with a high frequency in the text are more important or significant. Here we can find the words with the most significance for each president: 
```{r}
tidy_sotu %>%
  count(president, word)  %>%  # count n for each word
  group_by(president) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
          term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```
(@) The code above is commented to help you follow it. Walk through the code above, and explain what each line does in your own words. If it's a function you are unfamiliar with, look up the documentation.

> The first line uses the `count()` function to count how many times each president uses each unique word in their speeches. The second line groups this data by president. The third line uses `mutate()` to calculate the relative frequency of each unique word by dividing the total occurrences of the word by the total number of words in each document. To do this, `mutate()` creates two new columns: one called `n_tot` which contains the total number of words in each document, and the other called `term_freq` which contains the relative frequency of each word. The next line uses `arrange()` to sort the data by the values in the `term_freq` column, highest to lowest. The next line uses `top_n()` to select the top row for each president, that is, the most frequent word that each president uses. The final line prints all the rows selected in the previous line.

### TF-IDF (Term Frequency-Inverse Document Frequency)

The above measures the frequency of terms within individual documents. But what if we know about words that seem more important based on the contents of the **entire** corpus? That is where tf-idf or term-frequency according to inverse document frequency comes in. 

Tf-idf measures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term’s document frequency (number of documents within the corpus in which the term appears divided by the number of documents). The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. 

Don't worry too much about how tf-idf is calculated. But if you feel like you are a bit lost and want to understand the specifics - I recommend reading the [tf-idf wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and this blog post from [_Learn Data Science_](https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/).

We'll calculate tf-idf in the next code chunk but lets talk for a second about what that number will represent. It will be: 

    * lower for words that appear frequently in many documents of the corpus, and lowest when the word occurs in virtually all documents.
    * higher for words that appear frequently in just a few documents of the corpus, this lending high discriminatory power to those few documents.

Luckily, `tidytext` provides a function for calculating tf-idf. To calculate tf-idf the function needs a list of every word in every document and the count. Like this: 
```{r}
tidy_sotu %>%
  count(doc_id, word, sort = TRUE) # Count how many times each word appears in each document
```
We can feed that to the function and get the tf-idf: 
```{r}
sotu.tf.idf <- tidy_sotu %>%
  count(doc_id, word, sort = TRUE) %>% # Count how many times each word appears in each document
  bind_tf_idf(word, doc_id, n) # Calculate term frequency based on inverse document frequency

head(sotu.tf.idf)
```

The resulting data frame has 3 columns: term frequency (tf), inverse document frequency (idf) and Tf-idf (tf_idf).

Lets look at what the words with the highest tf-idf score are. 
```{r}
sotu.tf.idf %>% arrange(desc(tf_idf))
```

(@) Pick a president who served more than one term. Filter the dataset and generate both raw word counts and tf-idf scores for that president. What words are most significant in each method? Why and what does that tell you about that president? 

```{r}
tidy_sotu %>%
  filter(president == "George W. Bush") %>%
  count(doc_id, word, sort = TRUE) # count n for each word
```
> I chose President George W. Bush. When generating a raw word count, the words "america", "people", "security", "weapons", and "world" have the highest totals.

```{r}
tidy_sotu %>%
  filter(president == "George W. Bush") %>%
  count(doc_id, word, sort = TRUE) %>% # count n for each word  
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
          term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) # sort by term frequency
```

> The same set of words, with the addition of "budget", have the highest term frequencies relative to the total number of words in each speech.

```{r}
tidy_sotu %>%
  filter(president == "George W. Bush") %>%
  count(doc_id, word, sort = TRUE) %>% # count n for each word
  bind_tf_idf(word, doc_id, n) %>% # Calculate term frequency based on inverse document frequency
  arrange(desc(tf_idf))
```
> Interestingly, while words such as "america", "security", "people", "world", and "weapons" had both the highest raw word counts and highest term frequencies, they did not have high `tf_idf` values. This makes sense because words that appear frequently in many documents of the corpus will have lower `tf_idf` values. The words "empower", "hussein", "saddam", and "inspectors" have the highest tf_idf values, meaning they appear frequently in just a few documents of the corpus. "empower" appears in Bush's 2008 speech, while "hussein", "saddam" and "inspectors" appear in his 2003 speech. The latter makes sense because 2003 was when Bush ordered the invasion of Iraq to depose Saddam Hussein, who was said to have weapons of mass destruction (which UN inspectors had been sent to find).

> In summary, President George W. Bush frequently used the words "america", "security", "people", "world", and "weapons" through his speeches, but talked about Saddam Hussein in his 2003 speech more often than in all the other speeches. This reflects his political platform of nationalism, patriotism, and American intervention in the world, while also showing the significance of Saddam Hussein as America's enemy in 2003.

### Co-Occurance
Co-occurrence gives us a sense of words that appear in the same text, but not necessarily next to each other. It shows words that are likely to co-occur. Note that this is different than topic modeling, which we'll discuss next week. 

For this section we will make use of the `widyr` package. The function which helps us do this is the `pairwise_count()` function. It lets us count common pairs of words co-appearing within the same speech. This function might take a second as the resulting data frame will be incredibly large.

```{r}
sotu_word_pairs <- sotu_whole %>% 
  mutate(speech_end = word(text, -5000, end = -1)) %>%  # extract last 100 words
  unnest_tokens(word, speech_end) %>%   # tokenize
  filter(!word %in% stop_words$word) %>%  # remove stopwords
  pairwise_count(word, doc_id, sort = TRUE, upper = FALSE) # don't include upper triangle of matrix

head(sotu_word_pairs)
```

Now we have a list of words that appear near each other in the text as well as the frequency. Once again this dataset is far too large to look at in a data frame. Instead, we'll create a network graph that shows us the relationships between words for any words that appear more than 200 times. I chose 200 after looking at the above dataset and seeing that the highest count was 239. You want the network graph to be manageable and not too large. 
```{r}
library(igraph)
library(ggraph)

sotu_word_pairs %>% 
  filter(n >= 200) %>%  # only word pairs that occur 200 or more times
  graph_from_data_frame() %>% #convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```
(@) Create a network graph that shows the relationship between words that appear between 125 and 175 times.
```{r}
sotu_word_pairs %>% 
  filter(n >= 125 & n <= 175) %>%  # only word pairs that occur between 125 and 175 times
  graph_from_data_frame() %>% # convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "lightblue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

## Analyzing Historical Journals

In the github repository below I have included the text and metadata for a journal called _Mind and Body_ which ran from the 1890s until the late 1930s and chronicled the development of the physical education profession. This dataset was OCR'd from copies stored in Google Books. Using the metadata provided and the raw text files can you use what you learned above to analyze these texts? What historical conclusions might you be able to draw?

```{r}
#zip file of all the .txt files. One for each issue. 
download.file("https://github.com/regan008/8510-TextAnalysisData/blob/main/txt.zip?raw=true", "MindAndBody.zip")
unzip("MindAndBody.zip")

# Metadata that includes info about each issue.
metadata <- read.csv("https://raw.githubusercontent.com/regan008/8510-TextAnalysisData/main/Metadata.csv")
```

(@) Add code chunks below and intersperse text to explain what you are doing and why. 

> First we need to combine the metadata and texts of the Mind & Body issues into a single dataframe. We can do this by creating a character vector called `file_paths2` that contains the names of all the text files in the txt folder. Then we use the `readtext()` function to create a dataframe with two columns, one for the `doc_id` and one for the text. Since the values in `doc_id` all have the prefex `MB_`, which the values in the `Filename` column in `metadata` don't, I'm going to use `mutate()` and `gsub()` to get rid of it. Now that the `doc_id` column and `Filename` column have the exact same values, we can use `full_join` to join the two dataframs together into a single one called `mind.body`.

```{r}
file_paths2 <- list.files("txt/")
mind.body.texts <- readtext(paste("txt/", file_paths2, sep="")) %>%
  mutate(doc_id = gsub("MB_", "", doc_id))
 
mind.body <- full_join(metadata, mind.body.texts, by = c("Filename" = "doc_id")) %>%
  as_tibble() # convert to tibble for better screen viewing
```

> Now that we have all the data in a single dataframe, we can perform some pre-processing techniques to prepare it for analysis. First let's tokenize the dataframe by splitting the raw text into smaller units called tokens. We can use the function `unnest_tokens()` from the `tidytext` package to put each word on a separate row. It also cleans the text by converting all upper case letters to lower case and removing all special characters and punctuation. We can then filter out any rows containing numbers since these won't provide much insight.

```{r}
tidy.mind.body <- mind.body %>%
  unnest_tokens(word, text) %>% # Split text into words
  filter(!grepl('[0-9]', word)) # Remove numbers

tidy.mind.body
```
> Next we should remove "stop words" which are common or frequently used words that don't convey meaning about the content of the text. The `tidytext` package includes a default list of 1,149 stop words. We can remove them using the `anti_join()` function.

```{r}
tidy.mind.body <- tidy.mind.body %>%
  anti_join(stop_words)

tidy.mind.body
```
> It looks like there are still a few stop words that are common in this dataset that we need to add to our list. For example, "vol" is likely referring to the volume, "march" to the month, and "editorial" and "committee" to the editorial committee of the journal. Let's do a word count to see which are the most common.

```{r}
tidy.mind.body %>%
  count(word, sort = TRUE)
```

> Right away we see the word "digitized", which needs to be removed as it's related to the digitization of the texts, not their content. Others include "physical", "left", "dr" (unclear whether abbreviation for doctor or drive), "ooqle" (likely a misspelling of google), "google", "st" (abbreviation for street or Saint), "ft" (abbreviation for feet), "ii" (Roman numeral), "cjooqle" (misspelling of google), "pa" (possibly abbreviation for Pennsylvania), "wm" (abbreviation for William), "louis" (part of St. Louis), "wis", "iii", "june", "july", and stecher" (last name).

> Let's create a custom list of stop words, combine them with the default list, and remove them from the dataframe.

```{r}
stop_words_mb <- tibble(
  word = c(
    "vol",
    "march",
    "physical",
    "left",
    "digitized",
    "dr",
    "ooqle",
    "google",
    "st",
    "ft",
    "ii",
    "cjooqle",
    "pa",
    "wm",
    "louis",
    "wis",
    "iii",
    "june",
    "july",
    "stecher",
    "ave"
  ),
  lexicon = "Mind Body"
)

all_stop_words <- stop_words %>%
  bind_rows(stop_words_mb)

tidy.mind.body <- tidy.mind.body %>%
  anti_join(all_stop_words)
```

> Let's look at the most common words again.

```{r}
tidy.mind.body %>%
  count(word, sort = TRUE)
```

> This list doesn't tell us too much. It would be more meaningful if we looked at how many times each word appears in each issue of Mind and Body. We can do this by adding the filename which contains the month and year.

```{r}
tidy.mind.body %>%
  count(Filename, word, sort = TRUE)
```

> This still doesn't tell us a whole lot. There are some common placenames like "chicago" and "cleveland", while "health", "education", and "school" occur many times, unsurprisingly. It's time to start asking more specific questions of the data. For example, we could look at gender.

```{r}

gender.words <- c("boys", "girls", "boy", "girl", "men", "women", "man", "woman")
  
tidy.mind.body %>%
  filter(word %in% gender.words) %>% 
  count(word, sort = TRUE)
```

> This shows us that the word "boys" appears most frequently in the corpus, with "girls" a fairly close second. We can also see that the singular, "boy", is used much more frequently than "girl". What factors influenced this? We could look at the month and/or year of the issue to see if the journal targeted males or females at certain times of the year or if there was a change over time.

```{r}
tidy.mind.body %>%
  count(Month, word, sort = TRUE) %>%
  group_by(Month) %>% 
  mutate(n_tot = sum(n), # count total number of words per issue
         term_freq = n/n_tot) %>%
  arrange(desc(term_freq)) %>% # sort by term frequency
  filter(word %in% gender.words) %>%
  top_n(1) %>%  # take the top for each month
  print(n = Inf) # print all rows
```
> There doesn't seem to be a clear pattern here. The word "boys" is more frequent in January, February, March, May, May-June, June, July, August, September, and October, while "girls" is more frequent in
April, Sept-Oct, November, December, and Dec-Jan.

> It might help to look at which words often appear with "girls" or "boys" in the texts.

```{r}
mind.body.word.pairs <- mind.body %>%
  mutate(doc_end = word(text, -5000, end = -1)) %>%  # Extract last 100 words
  unnest_tokens(word, doc_end) %>%   # Tokenize
  filter(!grepl('[0-9]', word)) %>% # Remove numbers
  filter(!word %in% all_stop_words$word) %>%  # Remove default & custom stopwords
  pairwise_count(word, Filename, sort = TRUE, upper = FALSE)

mind.body.word.pairs %>%
  filter(grepl("girls|boys", item1) | grepl("girls|boys", item2)) # Show only word pairs containing "girls" or "boys"

```
> We can create a network graph to display the relationships between "girls" or "boys" and other words for any words that appear more than 250 times.

```{r}
mind.body.word.pairs %>%
  filter(grepl("girls|boys", item1) | grepl("girls|boys", item2)) %>%
  filter(n >= 250) %>%  # only word pairs that occur 250 or more times
  graph_from_data_frame() %>% #convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

> I haven't really drawn any historical conclusions from my analysis of gender in the data above, other than the fact that the word "boys" appears more frequently than "girls" in the corpus, suggesting a tendency towards more "masculine" themes. But that's still a tentative conclusion without knowing more about the journal. I think it would be helpful to have more metadata about the journal issues besides the year and month. Using that information, I found that "girls" appears more frequently than "boys" in the winter issues (November, December, and January), as well as in April, but I'm not sure if that is significant.

> I think I'll try a different angle and look at the most frequently mentioned sports.

```{r}
sports <- c("football", "basketball", "baseball", "swimming", "tennis", "rugby", "handball", "fencing", "golf", "hockey", "boxing", "polo", "wrestling", "cycling", "rowing", "skating")

tidy.mind.body %>% 
  filter(word %in% sports) %>% 
  count(word, sort = TRUE)
```
> This shows that swimming is the most commonly mentioned sport, followed by football, baseball, and tennis. We can use a line graph to see the change in frequency of a sports term over time, which might indicate changes in popularity.

```{r}
mind.body.sports <- tidy.mind.body %>%
  filter(word == "swimming" | word == "football" | word == "baseball" | word == "tennis") # Filter for most common sports

mind.body.sports %>%
  count(Year, word, sort = TRUE) %>%
  ggplot(aes(x = Year, y = n, color = word)) +
    geom_line() +
    labs(x="Year", y="Word Count") +
    guides(color = guide_legend(title = "Sport"))
```
> This graph shows that the word "swimming" appeared most frequently in the years between 1910 and 1920 (around 1903 to 1912), while "football" peaked around 1903-1905 before declining until after 1920. "Baseball" and "tennis" both followed a similar pattern. The usage of these sports terms could reflect the importance of the sports to the journal as well as the popularity of the sports among the journal's readership or in the United States generally.